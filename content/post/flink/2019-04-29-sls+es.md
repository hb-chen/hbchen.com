---
title: "ã€Flinkå®è·µã€‘å®æ—¶æ¶ˆè´¹é˜¿é‡Œäº‘SLSæ—¥å¿—+è¾“å‡ºåˆ°ES"
date: 2019-04-29T18:42:22+08:00
draft: false
comments: true
categories: [
	"å¤§æ•°æ®",
]
tags: [
	"Flink",
	"Elasticsearch",
	"SLS",
]
---
æœ¬æ–‡ä»‹ç»Flinkåœ¨æµè®¡ç®—åœºæ™¯çš„ç®€å•åº”ç”¨ï¼Œå®æ—¶æ¶ˆè´¹SLSå†…çš„AccessLogï¼Œå¹¶å°†è®¡ç®—ç»“æœè¾“å‡ºåˆ°Elasticsearchã€‚
<!--more-->

## ç¯å¢ƒ
- Flink
    - 1.8
- ES
    - 6.3.2

## å‡è®¾åœºæ™¯
*æ—¥å¿—ç»“æ„*
```bash
request_time: 1556415329
uri: /analysis/path?id=1
```
ç³»ç»ŸAccessLogé€šè¿‡Logtaié‡‡é›†åˆ°é˜¿é‡Œäº‘SLSï¼ŒæŒ‡å®š`request_time`ä¸ºæ—¥å¿—æ—¶é—´å­—æ®µã€‚ç¤ºä¾‹åœºæ™¯éœ€æ±‚ç»Ÿè®¡æŒ‡å®š`uri.path`+`uri.query.id`åˆ†ç»„ç»Ÿè®¡è®¿é—®è®¡æ•°ã€‚

- å®æ—¶æ¶ˆè´¹`SLS`æ—¥å¿—
- ç­›é€‰`uri.path`=`/analysis/path`(*ç®€åŒ–åªç»Ÿè®¡å•ä¸ª`path`ï¼Œå®é™…åœºæ™¯å¯èƒ½æ˜¯å¤šä¸ª`path`çš„è®¿é—®ç»Ÿè®¡ï¼Œç›¸åº”çš„`key`å‚æ•°è§„åˆ™ä¹Ÿä¼šä¸åŒ*)
- è§£æ`uri.query`å‚æ•°`id`

**åˆ›å»ºESç´¢å¼•**
```bash
PUT sls_analysis

PUT sls_analysis/_mapping/_doc
{
  "properties": {
    "path": {
      "type": "text"
    },
    "key": {
      "type": "integer"
    },
    "count": {
      "type": "integer"
    },
    "timestamp": {
      "type":"long"
    }
  }
}
```

## æµç¨‹è§£æ
æ¥ä¸‹æ¥ç»“åˆ[hb-chen/flink-practice](https://github.com/hb-chen/flink-practice/tree/master/sls)æºç å¯¹æµç¨‹è¿›è¡Œè§£æã€‚

### Gradleæ·»åŠ ä¾èµ–
```
    // ES
    flinkShadowJar "org.apache.flink:flink-connector-elasticsearch6_${scalaBinaryVersion}:${flinkVersion}"

    // SLS
    flinkShadowJar "com.aliyun.openservices:flink-log-connector:0.1.7"
    flinkShadowJar "com.aliyun.openservices:aliyun-log:0.6.19"
    flinkShadowJar "com.aliyun.openservices:log-loghub-producer:0.1.8"
    flinkShadowJar "com.google.protobuf:protobuf-java:2.5.0"
    
    // YAMLè§£æ
    flinkShadowJar "org.yaml:snakeyaml:1.24"
```
### å‚æ•°&é…ç½®ï¼ˆå¯å¿½ç•¥ï¼‰
è™½ç„¶åªæ˜¯ç¤ºä¾‹è¿˜æ˜¯åšäº†`args`å‚æ•°åŠ`config.yml`é…ç½®çš„è·å–ï¼Œæ–¹ä¾¿é…ç½®
```java
public static void main(String[] args) throws Exception {
    final ParameterTool params = ParameterTool.fromArgs(args);
    String analysisPath = params.getRequired("path");
    String analysisQueryKey = params.get("key", "id");
    LOG.info("access log analysis path:" + analysisPath + " key:" + analysisQueryKey);

    Config config = getConfig();
    
    // â€¦â€¦
}

private static Config getConfig() {
    Constructor constructor = new Constructor(Config.class);
    org.yaml.snakeyaml.Yaml yaml = new org.yaml.snakeyaml.Yaml(constructor);

    Config config = yaml.loadAs(SlsAnalysis.class.getClassLoader().getResourceAsStream("config.yml"), Config.class);
    return config;
}
```

### æ—¥å¿—æ¶ˆè´¹
é¦–å…ˆå‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://help.aliyun.com/document_detail/63594.html)é€šè¿‡`SLS`çš„å„ç§é…ç½®åˆ›å»ºä¸€ä¸ª`Consumer`ï¼Œè·å¾—`SLS`æ—¥å¿—æµ`DataStream<RawLogGroupList> logStream`
```java
    // SLSæ¶ˆè´¹
    // https://help.aliyun.com/document_detail/63594.html
    Properties configProps = new Properties();
    // è®¾ç½®è®¿é—®æ—¥å¿—æœåŠ¡çš„åŸŸå
    configProps.put(ConfigConstants.LOG_ENDPOINT, config.getSls().getEndpoint());
    // è®¾ç½®è®¿é—®ak
    configProps.put(ConfigConstants.LOG_ACCESSSKEYID, config.getSls().getAk());
    configProps.put(ConfigConstants.LOG_ACCESSKEY, config.getSls().getSk());
    // è®¾ç½®æ—¥å¿—æœåŠ¡çš„project
    configProps.put(ConfigConstants.LOG_PROJECT, config.getSls().getProject());
    // è®¾ç½®æ—¥å¿—æœåŠ¡çš„Logstore
    configProps.put(ConfigConstants.LOG_LOGSTORE, config.getSls().getLogStore());
    // è®¾ç½®æ¶ˆè´¹æ—¥å¿—æœåŠ¡èµ·å§‹ä½ç½®
    configProps.put(ConfigConstants.LOG_CONSUMER_BEGIN_POSITION, "" + (System.currentTimeMillis() / 1000L));
    // è®¾ç½®æ—¥å¿—æ‹‰å–æ—¶é—´é—´éš”åŠæ¯æ¬¡è°ƒç”¨æ‹‰å–çš„æ—¥å¿—æ•°é‡
    configProps.put(ConfigConstants.LOG_FETCH_DATA_INTERVAL_MILLIS, "1000");
    configProps.put(ConfigConstants.LOG_MAX_NUMBER_PER_FETCH, "100");
    // è®¾ç½®Shardså‘ç°å‘¨æœŸ
    configProps.put(ConfigConstants.LOG_SHARDS_DISCOVERY_INTERVAL_MILLIS, Consts.DEFAULT_SHARDS_DISCOVERY_INTERVAL_MILLIS);

    // è®¾ç½®æ—¥å¿—æœåŠ¡çš„æ¶ˆæ¯ååºåˆ—åŒ–æ–¹æ³•
    RawLogGroupListDeserializer deserializer = new RawLogGroupListDeserializer();
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    DataStream<RawLogGroupList> logStream = env.addSource(new FlinkLogConsumer<RawLogGroupList>(deserializer, configProps));
```

é€šè¿‡`RawLogGroupList`ç»“æ„çŸ¥é“`logStream`è·å¾—çš„æ˜¯æ‰¹é‡çš„`SLS`æ—¥å¿—ï¼Œå•æ¡æ—¥å¿—æ˜¯åˆ†ç»„çš„`RawLog`åˆ—è¡¨ï¼Œè¿™å°±éœ€è¦é¦–å…ˆå¯¹æ‰¹é‡çš„æ•°æ®è¿›è¡Œå±•å¼€(`flatMap()`)æ¥è·å¾—å•æ¡æ•°æ®æµ
```java
public class RawLogGroupList implements Serializable {
    public List<RawLogGroup> rawLogGroups = new ArrayList();
}

public class RawLogGroup implements Serializable {
    public String source;
    public String topic = "";
    public Map<String, String> tags = new HashMap();
    public List<RawLog> logs = new ArrayList();
}
```

### æ‰¹é‡æ—¥å¿—å±•å¼€
è¿™æ˜¯æ•´ä¸ªè¿‡æµç¨‹æœ€å…³é”®çš„ä¸€ç¯ï¼Œè¿™é‡Œè¦å®šä¹‰å±•å¼€åæ•°æ®æµçš„ç»“æ„ï¼Œä¸å•°å—¦ç›´æ¥çœ‹ç»“æ„`Tuple4<AccessLogAnalysis, Integer, Long, Long>`

- `f0`:`AccessLogAnalysis`æ˜¯è§£æ`uri`çš„`path`+`key`çš„å¯¹è±¡ï¼Œä¹Ÿå°±æ˜¯è¦ç”¨åš`keyBy`çš„å±æ€§
- `f1`:`Integer`ç»Ÿè®¡æ•°é‡ï¼Œå•æ¡æ—¥å¿—å…¨è®¡ä¸º1(*æ ¹æ®éœ€æ±‚è¿™é‡Œå¯ä»¥è€ƒäº›ä¼˜åŒ–ï¼Œæ¯”å¦‚Groupå†…æœ‰åºå¯ä»¥åœ¨rangeè¿‡ç¨‹ç›´æ¥å¯¹ç›¸åŒ`request_time`åšèšåˆ*)
- `f2`:`Long`æ—¥å¿—æ—¶é—´æˆ³ï¼Œè€ƒè™‘åç»­ä½œä¸º`EventTime`ä½¿ç”¨ï¼Œæ‰€ä»¥å°†`request_time`è½¬ä¸ºæ¯«ç§’
- `f3`:`Long`æ‰¹é‡æ—¥å¿—çš„æœ€å°æ—¶é—´æˆ³ï¼Œå› ä¸º`SLS`æ—¥å¿—æ˜¯æ—¶é—´æœ‰åºçš„ï¼Œä¸”å•ä¸ª`RawLogGroup`æ—¥å¿—å‡åºï¼Œæ‰€ä»¥è‡ªç„¶çš„è€ƒè™‘ä½¿ç”¨æœ€å°æ—¶é—´æˆ³ä½œä¸º`watermark`ï¼Œè¿™æ ·ä¹Ÿé¿å…äº†**arrive late**çš„å‡ºç°

```java
    // SLSæ‰¹é‡æ—¥å¿—å±•å¼€
    DataStream<Tuple4<AccessLogAnalysis, Integer, Long, Long>> flatStream = logStream.flatMap(new FlatMapFunction<RawLogGroupList, Tuple4<AccessLogAnalysis, Integer, Long, Long>>() {
        @Override
        public void flatMap(RawLogGroupList value, Collector<Tuple4<AccessLogAnalysis, Integer, Long, Long>> out) throws Exception {
            // Log groupå†…æ—¶é—´å‡åºï¼Œè®°å½•æ‰€æœ‰åˆ†ç»„æœ€å°æ—¶é—´æˆ³ï¼Œå³ä¸ºwatermarkä½ç½®
            long minTimestamp = Long.MAX_VALUE;
            for (RawLogGroup group : value.getRawLogGroups()) {
                if (group.getLogs().size() > 0) {
                    RawLog log = group.getLogs().get(0);
                    long rt = Optional.ofNullable(log.getContents().get("request_time")).map(Long::new).orElse(Long.MAX_VALUE);
                    minTimestamp = rt < minTimestamp ? rt : minTimestamp;
                }
            }
            // å–æ¯«ç§’ï¼Œæ’é™¤MAX_VALUE
            minTimestamp = minTimestamp == Long.MAX_VALUE ? minTimestamp : minTimestamp * 1000;

            Integer count = 0;
            for (RawLogGroup group : value.getRawLogGroups()) {
                count += group.getLogs().size();
                for (RawLog log : group.getLogs()) {
                    // URIè§£æ
                    QueryStringDecoder decoder = new QueryStringDecoder(log.getContents().get("uri"));
                    String path = decoder.path();
                    List<String> keyList = decoder.parameters().get(analysisQueryKey);
                    if (path.equalsIgnoreCase(analysisPath) && keyList != null && keyList.size() > 0) {
                        Integer id = Optional.ofNullable(keyList.get(0)).map(Integer::new).orElse(0);
                        AccessLogAnalysis ala = new AccessLogAnalysis();
                        ala.setKey(id);
                        ala.setPath(path);

                        Long timestamp = Optional.ofNullable(log.getContents().get("request_time")).map(Long::new).orElse(0L);
                        timestamp *= 1000;

                        out.collect(new Tuple4<>(ala, 1, timestamp, minTimestamp));
                    }

                }
            }
            LOG.info("raw log count:" + count + " timestamp:" + minTimestamp);
        }
    });
```

### EventTime & timeWindow
æœ‰äº†`flatStream`çš„ç»“æœï¼Œè‡ªå®šä¹‰`EventTime`ä»¥åŠ`Watermark`å°±æ¯”è¾ƒç®€å•ã€‚<br/>`.keyBy(0).timeWindow(Time.seconds(60)).sum(1)`æµç¨‹ä»¥`f0`ä¸º`key`åˆ†éš”ï¼Œ60sä¸ºæ—¶é—´çª—å£å¯¹`f1`æ±‚å’Œã€‚
```java
DataStream result = flatStream.assignTimestampsAndWatermarks(new AssignerWithPunctuatedWatermarks<Tuple4<AccessLogAnalysis, Integer, Long, Long>>() {
    @Nullable
    @Override
    public Watermark checkAndGetNextWatermark(Tuple4<AccessLogAnalysis, Integer, Long, Long> lastElement, long extractedTimestamp) {
        return lastElement.f3 <= extractedTimestamp ? new Watermark(lastElement.f3) : null;
    }

    @Override
    public long extractTimestamp(Tuple4<AccessLogAnalysis, Integer, Long, Long> element, long previousElementTimestamp) {
        if (element.f2 > 0L) {
            return element.f2;
        } else {
            return previousElementTimestamp;
        }
    }
})
.keyBy(0)
.timeWindow(Time.seconds(60))
.sum(1);
```

### è¾“å‡ºåˆ°ES
å¤§è‡´å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/connectors/elasticsearch.html)å³å¯ï¼Œ
åªæ˜¯æ²¡æœ‰è®¤è¯(*ç”Ÿäº§ç¯å¢ƒå¿…ä¸å¯å°‘ï¼Œæƒ³åˆ°ESç¤¾åŒºçš„åˆ†äº«ï¼Œåœ¨[shodan.io](https://www.shodan.io)ä¸Šæ‰¾å„ç§å…¬å¼€å…è´¹ESèµ„æºğŸ˜*)
```java
// Sink:Elasticsearch
List<HttpHost> httpHosts = new ArrayList<>();
httpHosts.add(new HttpHost(config.getEs().getHostname(), 9200, "http"));

// use a ElasticsearchSink.Builder to create an ElasticsearchSink
ElasticsearchSink.Builder<Tuple4<AccessLogAnalysis, Integer, Long, Long>> esSinkBuilder = new ElasticsearchSink.Builder<>(httpHosts, new ElasticsearchSinkFunction<Tuple4<AccessLogAnalysis, Integer, Long, Long>>() {
    public IndexRequest createIndexRequest(Tuple4<AccessLogAnalysis, Integer, Long, Long> element) {
        Map<String, String> json = new HashMap<>();
        json.put("path", element.f0.getPath());
        json.put("key", element.f0.getKey().toString());
        json.put("count", element.f1.toString());
        json.put("timestamp", element.f3.toString());

        return Requests.indexRequest().index("sls_analysis").type("_doc").source(json);
    }

    @Override
    public void process(Tuple4<AccessLogAnalysis, Integer, Long, Long> element, RuntimeContext runtimeContext, RequestIndexer requestIndexer) {
        requestIndexer.add(createIndexRequest(element));
    }
});

// configuration for the bulk requests; this instructs the sink to emit after every element, otherwise they would be buffered
esSinkBuilder.setBulkFlushMaxActions(1);

// provide a RestClientFactory for custom configuration on the internally created REST client
String esUsername = config.getEs().getUsername();
String esPassword = config.getEs().getPassword();
esSinkBuilder.setRestClientFactory(restClientBuilder -> {
    // restClientBuilder.setDefaultHeaders(headers);
    // restClientBuilder.setMaxRetryTimeoutMillis(...)
    // restClientBuilder.setPathPrefix(...)
    restClientBuilder.setHttpClientConfigCallback(new RestClientBuilder.HttpClientConfigCallback() {
        @Override
        public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpAsyncClientBuilder) {
            CredentialsProvider credentialsProvider = new BasicCredentialsProvider();
            credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(esUsername, esPassword));
            return httpAsyncClientBuilder.setDefaultCredentialsProvider(credentialsProvider);
        }
    });
});

// finally, build and add the sink to the job's pipeline
result.addSink(esSinkBuilder.build());
```

è¿™é‡Œè¿˜é‡åˆ°ä¸ªå°å‘ï¼ˆJavaä¸¢çš„å¤ªä¹…äº†ğŸ˜‚ï¼‰ï¼Œåœ¨`UsernamePasswordCredentials()`ä¸­ç›´æ¥ä½¿ç”¨`config.getEs().getUsername()` `config.getEs().getPassword()`ï¼Œå¯¼è‡´åºåˆ—åŒ–é”™è¯¯ï¼Œç›¸å…³å‚è€ƒ[Apache Flink#è¸©è¿‡çš„å‘](https://yuzhouwan.com/posts/20644/)
```java
Exception in thread "main" org.apache.flink.api.common.InvalidProgramException: The implementation of the ElasticsearchSinkBase is not serializable. The object probably contains or references non serializable fields.
```
### æµ‹è¯• & æ‰“åŒ…
```bash
# æµ‹è¯•
$ ./gradlew sls:run --args="--path /analysis/path"

# æ‰“åŒ…
$ ./gradlew clean sls:shadowJar
```

### æäº¤Jobåˆ°Flinkè¿è¡Œ
ä¸Šä¼ `.jar`åˆ°Flink
```java
Program Arguments
--path /analysis/path
```
![auth-adapter](/img/flink/sls+es-dashboard.png)

### åˆ°Kibanaçœ‹ä¸‹ç»“æœ

![auth-adapter](/img/flink/sls+es-kibana.png)

```bash
GET /sls_analysis/_search
{
  "aggs": {
    "2": {
      "terms": {
        "field": "key",
        "size": 20,
        "order": {
          "1": "desc"
        }
      },
      "aggs": {
        "1": {
          "sum": {
            "field": "count"
          }
        }
      }
    }
  },
  "size": 0,
  "_source": {
    "excludes": []
  },
  "stored_fields": [
    "*"
  ],
  "script_fields": {},
  "docvalue_fields": [],
  "query": {
    "bool": {
      "must": [
        {
          "match_all": {}
        },
        {
          "match_phrase": {
            "path": {
              "query": "/analysis/path"
            }
          }
        }
      ],
      "filter": [],
      "should": [],
      "must_not": []
    }
  }
}
```

## TODO

- Checkpoint
